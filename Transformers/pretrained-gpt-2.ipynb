{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n!pip install tiktoken","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T11:04:03.086649Z","iopub.execute_input":"2024-11-18T11:04:03.087420Z","iopub.status.idle":"2024-11-18T11:04:17.727557Z","shell.execute_reply.started":"2024-11-18T11:04:03.087367Z","shell.execute_reply":"2024-11-18T11:04:17.726419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom dataclasses import dataclass\nimport torch.nn.functional as F\nimport time\nimport math\nimport inspect","metadata":{"execution":{"iopub.status.busy":"2024-11-18T12:08:45.823650Z","iopub.execute_input":"2024-11-18T12:08:45.824020Z","iopub.status.idle":"2024-11-18T12:08:45.829330Z","shell.execute_reply.started":"2024-11-18T12:08:45.823985Z","shell.execute_reply":"2024-11-18T12:08:45.828249Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tiktoken\nenc = tiktoken.get_encoding('gpt2')\n\nclass DataLoaderLite:\n    def __init__(self,B, T):\n        self.B = B\n        self.T = T\n        with open('input.txt') as f:\n            text = f.read()\n        enc = tiktoken.get_encoding('gpt2')\n        tokens = enc.encode(text)\n        self.tokens = torch.tensor(tokens, dtype=torch.long)\n        print(f\"loaded {len(self.tokens)} tokens\")\n        print(f\"1 epoch = {len(self.tokens) // (B*T)}\")\n        self.current_position = 0\n        \n    def next_batch(self):\n        current_tokens = self.tokens[self.current_position: self.current_position + self.B*self.T+1]\n        x = current_tokens[:-1].view(self.B, self.T)\n        y = current_tokens[1:].view(self.B, self.T)\n        self.current_position += self.B * self.T\n        if self.current_position + self.B * self.T + 1 > len(self.tokens):\n            self.current_position = 0\n        return x, y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T11:04:21.612308Z","iopub.execute_input":"2024-11-18T11:04:21.612711Z","iopub.status.idle":"2024-11-18T11:04:22.637330Z","shell.execute_reply.started":"2024-11-18T11:04:21.612678Z","shell.execute_reply":"2024-11-18T11:04:22.636545Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n        self.gelu = nn.GELU(approximate='tanh')\n        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n        self.c_proj.NANOGPT_SCALE_INIT = 1\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = self.gelu(x)\n        return self.c_proj(x)\n\nclass CausalSelfAttetion(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=True)\n        self.c_proj = nn.Linear(config.n_embd,config.n_embd, bias=True)\n        self.c_proj.NANOGPT_SCALE_INIT = 1\n        self.register_buffer('bias', torch.triu(torch.ones(config.block_size, config.block_size), diagonal=1).view(1, 1, config.block_size, config.block_size))\n    \n    def forward(self, x):\n        B, T, C = x.size()\n        qkv = self.c_attn(x)\n        q, k, v = qkv.split(self.config.n_embd, dim = 2)\n        q = q.view(B, T, self.config.n_head, C // self.config.n_head).transpose(1, 2)\n        k = k.view(B, T, self.config.n_head, C // self.config.n_head).transpose(1, 2)\n        v = v.view(B, T, self.config.n_head, C // self.config.n_head).transpose(1, 2)\n        \n        # att = q @ k.transpose(-2, -1) * (k.size(-1) ** -0.5)\n        # att = att.masked_fill(self.bias[:,:,:T,:T] == 1,  float(\"-inf\"))\n        # probs = F.softmax(att, dim=-1)\n        # y = probs @ v\n\n        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n        \n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n        y = self.c_proj(y)\n        return y\n\nclass Block(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.ln_1 = nn.LayerNorm(config.n_embd)\n        self.attn = CausalSelfAttetion(config)\n        self.ln_2 = nn.LayerNorm(config.n_embd)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\n@dataclass\nclass GPTConfig:\n    block_size: int = 1024\n    vocab_size: int = 50688\n    n_layer: int = 12\n    n_head: int = 12\n    n_embd: int = 768\n    \nclass GPT(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size, config.n_embd),\n            wpe = nn.Embedding(1024, config.n_embd),\n            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            ln_f = nn.LayerNorm(config.n_embd)\n        ))\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n\n        #weight sharing scheme\n        self.transformer.wte.weight = self.lm_head.weight\n    \n        # self.apply iterates through all modules and applyes self._init_weights function\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            std = 0.02\n            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n                std *= (2 * self.config.n_layer) ** -0.5\n            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n        \n    def forward(self, x, targets=None):\n        B, T = x.size()\n        assert T <= self.config.block_size, f\"Cannot forward the sequence of length {T}, block_size is smaller\"\n        pos = torch.arange(0, T, dtype=torch.long, device=x.device)\n        pos_embd = self.transformer.wpe(pos)\n        tok_embd = self.transformer.wte(x)\n        x = pos_embd + tok_embd\n\n        for block in self.transformer.h:\n            x = block(x)\n        x = self.transformer.ln_f(x)\n        logits = self.lm_head(x)\n\n        loss = None\n        if targets is not None:\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n        return logits, loss\n\n    def configure_optimizers(self, weight_decay, learning_rate, device):\n        # start with all of the candidate parameters (that require grad)\n        param_dict = {pn: p for pn, p in self.named_parameters()}\n        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n        optim_groups = [\n            {'params': decay_params, 'weight_decay': weight_decay},\n            {'params': nodecay_params, 'weight_decay': 0.0}\n        ]\n        num_decay_params = sum(p.numel() for p in decay_params)\n        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n        # Create AdamW optimizer and use the fused version if it is available\n        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n        use_fused = fused_available and 'cuda' in (device.type, )\n        print(f\"using fused AdamW: {use_fused}\")\n        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n        return optimizer\n\n    @classmethod\n    def from_pretrained(cls, model_type):\n        assert model_type in ('gpt2', 'gpt2-medium', 'gpt2-large', 'gpt-xl')\n        from transformers import GPT2LMHeadModel\n        print(\"Loading weight from pretrained %s\" % model_type)\n    \n        config_args = {\n            'gpt2': dict(n_layer=12, n_head=12, n_embd=768),\n            'gpt2-medium': dict(n_layer=24, n_head=16, n_embd=1024), \n            'gpt2-large': dict(n_layer=36, n_head=20, n_embd=1280),\n            'gpt2-xl': dict(n_layer=48, n_head=25, n_embd=1600)\n        }[model_type]    \n        config_args['vocab_size'] = 50257\n        config_args['block_size'] = 1024\n        \n        config = GPTConfig(**config_args)\n        model = GPT(config)\n        sd = model.state_dict()\n        sd_keys = sd.keys()\n        sd_keys = [key for key in sd_keys if not key.endswith('.attn.bias')]\n        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n        sd_hf = model_hf.state_dict()\n\n        sd_keys_hf = sd_hf.keys()\n        sd_keys_hf = [key for key in sd_keys_hf if not key.endswith('.attn.bias')]\n        sd_keys_hf = [key for key in sd_keys_hf if not key.endswith('.attn.masked_bias')]\n        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n\n        assert len(sd_keys) == len(sd_keys_hf), f\"{len(sd_keys)} != {len(sd_keys_hf)}\"\n\n        for k in sd_keys_hf:\n            if any(k.endswith(w) for w in transposed):\n                assert sd[k].shape == sd_hf[k].shape[::-1]\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k].t())\n            else:\n                assert sd[k].shape == sd_hf[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k])\n\n        return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T12:14:01.889575Z","iopub.execute_input":"2024-11-18T12:14:01.890121Z","iopub.status.idle":"2024-11-18T12:14:01.930067Z","shell.execute_reply.started":"2024-11-18T12:14:01.890080Z","shell.execute_reply":"2024-11-18T12:14:01.929178Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(50257, 52000):\n    ans = 0\n    tmp = i\n    while tmp % 2 == 0:\n        ans += 1\n        tmp = tmp // 2\n    if ans > 7:\n        print(i, ans)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T12:08:20.855069Z","iopub.execute_input":"2024-11-18T12:08:20.855399Z","iopub.status.idle":"2024-11-18T12:08:20.862241Z","shell.execute_reply.started":"2024-11-18T12:08:20.855338Z","shell.execute_reply":"2024-11-18T12:08:20.861413Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"50688 / 512","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T11:35:26.781335Z","iopub.execute_input":"2024-11-18T11:35:26.781808Z","iopub.status.idle":"2024-11-18T11:35:26.787798Z","shell.execute_reply.started":"2024-11-18T11:35:26.781768Z","shell.execute_reply":"2024-11-18T11:35:26.786886Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"total_batch_size = 524288 # 2**19, ~0.5M, in number of tokens\nB = 16 # micro batch size\nT = 1024 # sequence length\nassert total_batch_size % (B * T) == 0, \"make sure total_batch_size is divisible by B * T\"\ngrad_accum_steps = total_batch_size // (B * T)\nprint(f\"total desired batch size: {total_batch_size}\")\nprint(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T12:32:05.830321Z","iopub.execute_input":"2024-11-18T12:32:05.830756Z","iopub.status.idle":"2024-11-18T12:32:05.836928Z","shell.execute_reply.started":"2024-11-18T12:32:05.830714Z","shell.execute_reply":"2024-11-18T12:32:05.835990Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#model = GPT.from_pretrained('gpt2')\n\ntorch.manual_seed(1337)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(1337)\n    \ntrain_loader = DataLoaderLite(B=4, T=1024)\n#torch.set_float32_matmul_precision('high')\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = GPT(GPTConfig())\n#model = torch.compile(model)\nif torch.cuda.device_count() > 1:\n    print(f\"Используем {torch.cuda.device_count()} GPU\")\n    # Оборачиваем модель в DataParallel\n    model = nn.DataParallel(model)\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T13:07:04.804617Z","iopub.execute_input":"2024-11-18T13:07:04.805244Z","iopub.status.idle":"2024-11-18T13:07:07.948334Z","shell.execute_reply.started":"2024-11-18T13:07:04.805201Z","shell.execute_reply":"2024-11-18T13:07:07.947335Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_lr = 6e-4\nmin_lr = max_lr * 0.1\nwarmup_steps = 10\nmax_steps = 200\ntotal_iterations=500\ndef get_lr(it):\n    # 1) linear warmup for warmup_iters steps\n    if it < warmup_steps:\n        return max_lr * (it+1) / warmup_steps\n    # 2) if it > lr_decay_iters, return min learning rate\n    if it > max_steps:\n        return min_lr\n    # 3) in between, use cosine decay down to min learning rate\n    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n    assert 0 <= decay_ratio <= 1\n    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n    return min_lr + coeff * (max_lr - min_lr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T13:07:07.950171Z","iopub.execute_input":"2024-11-18T13:07:07.950520Z","iopub.status.idle":"2024-11-18T13:07:07.957415Z","shell.execute_reply.started":"2024-11-18T13:07:07.950484Z","shell.execute_reply":"2024-11-18T13:07:07.956390Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = model.module.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device=device)\nfor step in range(total_iterations):\n    t0 = time.time()\n    optimizer.zero_grad()\n    loss_accum = 0.0\n    for grad_step in range(grad_accum_steps):\n        x, y = train_loader.next_batch()\n        x, y = x.to(device), y.to(device)\n        # with torch.autocast(device_type=device.type, dtype=torch.float16):\n        #     logits, loss = model(x, y)\n        logits, loss = model(x, y)\n        loss = loss / grad_accum_steps\n        if loss.dim() > 0:\n            loss = loss.mean()\n        loss_accum += loss.detach()\n        loss.backward()\n        \n    norm = torch.nn.utils.clip_grad_norm_(model.module.parameters(), 1.0)\n    lr = get_lr(step)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr  \n    optimizer.step()\n    torch.cuda.synchronize()\n    t1 = time.time()\n    dt = (t1 - t0)\n    tokens_per_sec = (train_loader.B * train_loader.T * grad_accum_steps) / (t1 - t0)\n    \n    print(f\"step {step:4d} | loss: {loss_accum.item():.4f} | norm: {norm.item():.2f} | dt: {dt:.2f}s | tok/sec: {tokens_per_sec:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T13:07:07.958699Z","iopub.execute_input":"2024-11-18T13:07:07.958998Z","iopub.status.idle":"2024-11-18T15:09:50.501139Z","shell.execute_reply.started":"2024-11-18T13:07:07.958964Z","shell.execute_reply":"2024-11-18T15:09:50.499699Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_params = 0\nlm_head = model.lm_head.weight.size(0) * model.lm_head.weight.size(1)\nfor param in model.parameters():\n    num_params += param.numel()\n\nprint(f\"total_number of parameters: {num_params}\")\nprint(f\"number of parameters in head linear layer {lm_head}\")\nprint(f\"proportion of parameters: {(lm_head / num_params):.4f}\")\n#print(f\"new total number of parameters after sharing wte and lm_head {num_params - lm_head}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T11:22:14.491453Z","iopub.execute_input":"2024-11-18T11:22:14.492318Z","iopub.status.idle":"2024-11-18T11:22:14.499791Z","shell.execute_reply.started":"2024-11-18T11:22:14.492275Z","shell.execute_reply":"2024-11-18T11:22:14.498806Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_length = 300\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n\nx = torch.zeros((1,1), dtype=torch.long).to(device)\n\nwhile x.size(1) < max_length:\n    with torch.no_grad():\n        logits, loss = model(x)\n        logits = logits[:, -1, :]\n        probs = F.softmax(logits, dim=-1)\n        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n        ix = torch.multinomial(topk_probs, 1)\n        new_token = torch.gather(topk_indices, -1, ix)\n        x = torch.cat([x, new_token], dim=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T15:10:38.054844Z","iopub.execute_input":"2024-11-18T15:10:38.055513Z","iopub.status.idle":"2024-11-18T15:10:44.746984Z","shell.execute_reply.started":"2024-11-18T15:10:38.055470Z","shell.execute_reply":"2024-11-18T15:10:44.745928Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for sample in x:\n    print(enc.decode(sample.cpu().tolist()), end='\\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T15:10:44.748642Z","iopub.execute_input":"2024-11-18T15:10:44.748982Z","iopub.status.idle":"2024-11-18T15:10:44.755166Z","shell.execute_reply.started":"2024-11-18T15:10:44.748945Z","shell.execute_reply":"2024-11-18T15:10:44.754193Z"}},"outputs":[],"execution_count":null}]}