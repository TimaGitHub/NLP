{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n!pip install einops","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T09:59:31.513516Z","iopub.execute_input":"2024-11-07T09:59:31.514290Z","iopub.status.idle":"2024-11-07T09:59:45.853192Z","shell.execute_reply.started":"2024-11-07T09:59:31.514245Z","shell.execute_reply":"2024-11-07T09:59:45.852045Z"}},"outputs":[{"name":"stdout","text":"--2024-11-07 09:59:32--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1115394 (1.1M) [text/plain]\nSaving to: 'input.txt'\n\ninput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.02s   \n\n2024-11-07 09:59:32 (46.3 MB/s) - 'input.txt' saved [1115394/1115394]\n\nCollecting einops\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: einops\nSuccessfully installed einops-0.8.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"with open('/kaggle/working/input.txt', 'r') as f:\n    text = f.read()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T09:59:45.855080Z","iopub.execute_input":"2024-11-07T09:59:45.855408Z","iopub.status.idle":"2024-11-07T09:59:45.861675Z","shell.execute_reply.started":"2024-11-07T09:59:45.855366Z","shell.execute_reply":"2024-11-07T09:59:45.860744Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"chars = sorted(list(set(text)))\nvocab_size = len(chars)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T09:59:45.862786Z","iopub.execute_input":"2024-11-07T09:59:45.863089Z","iopub.status.idle":"2024-11-07T09:59:45.888930Z","shell.execute_reply.started":"2024-11-07T09:59:45.863058Z","shell.execute_reply":"2024-11-07T09:59:45.888055Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"stoi = {i: j for i, j in enumerate(chars)}\nitos = {j: i for i, j in enumerate(chars)}\n\nencode = lambda input: [itos[ch] for ch in input]\ndecode = lambda input: ''.join([stoi[ch] for ch in input])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T09:59:45.891013Z","iopub.execute_input":"2024-11-07T09:59:45.891306Z","iopub.status.idle":"2024-11-07T09:59:45.899502Z","shell.execute_reply.started":"2024-11-07T09:59:45.891275Z","shell.execute_reply":"2024-11-07T09:59:45.898677Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch\ndata = torch.tensor(encode(text), dtype = torch.long)\nprint(data.shape, data.dtype)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T09:59:45.900499Z","iopub.execute_input":"2024-11-07T09:59:45.900807Z","iopub.status.idle":"2024-11-07T09:59:49.378917Z","shell.execute_reply.started":"2024-11-07T09:59:45.900775Z","shell.execute_reply":"2024-11-07T09:59:49.377923Z"}},"outputs":[{"name":"stdout","text":"torch.Size([1115394]) torch.int64\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"n = int(0.9 * len(data))\ntrain_data = data[:n]\nval_data = data[n:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T09:59:49.380098Z","iopub.execute_input":"2024-11-07T09:59:49.380533Z","iopub.status.idle":"2024-11-07T09:59:49.393669Z","shell.execute_reply.started":"2024-11-07T09:59:49.380499Z","shell.execute_reply":"2024-11-07T09:59:49.392775Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"torch.manual_seed(1337)\nbatch_size = 16\nblock_size = 32\n\ndef get_batch(split):\n    data = train_data if split == 'train' else val_data\n\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    \n    return x, y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T20:57:22.526618Z","iopub.execute_input":"2024-11-06T20:57:22.527022Z","iopub.status.idle":"2024-11-06T20:57:22.534843Z","shell.execute_reply.started":"2024-11-06T20:57:22.526984Z","shell.execute_reply":"2024-11-06T20:57:22.533882Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            X = X.to(device)\n            Y = Y.to(device)\n            logits, loss = model(X, targets=Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T12:07:03.102880Z","iopub.execute_input":"2024-11-07T12:07:03.103252Z","iopub.status.idle":"2024-11-07T12:07:03.110181Z","shell.execute_reply.started":"2024-11-07T12:07:03.103216Z","shell.execute_reply":"2024-11-07T12:07:03.109249Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmax_iters = 5000\neval_interval = 100\nlr = 1e-3\neval_iters = 200\nn_embd = 64\nn_head = 4\nn_layer = 4\ndropout = 0.0\n\nclass FeedForward(nn.Module):\n    def __init__(self, n_embd): \n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(n_embd, n_embd * 4), \n                                 nn.ReLU(),\n                                 nn.Linear(4 * n_embd, n_embd), \n                                 nn.Dropout(dropout)\n                                )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self): \n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, n_embd)\n        self.pos_enc = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.lm_f = nn.LayerNorm(n_embd)\n        self.lm_layer = nn.Linear(n_embd, vocab_size)\n        \n    def forward(self, idx, targets=None):\n        # idx shape (B, T)\n        B, T = idx.shape\n        token_embd = self.embed(idx)\n        pos_embd = self.pos_enc(torch.arange(T, device=device))\n        x = token_embd + pos_embd\n        x = self.blocks(x)\n        x = self.lm_f(x)\n        logits = self.lm_layer(x)\n        \n        if targets is None:\n            loss = None\n        else:\n            #logits shape (B, T), targets shape (B, T)\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -block_size:]\n            logits, loss = self(idx_cond)\n            logits = logits[:,-1,:]\n            probs = F.softmax(logits, dim=1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat([idx, idx_next], dim=1)\n\n        return idx\n\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n    def forward(self, x):\n        # input of size (batch, time-step, channels)\n        # output of size (batch, time-step, head size)\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,hs)\n        q = self.query(x) # (B,T,hs)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,hs)\n        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n        return out\n        \n        \nclass Block(nn.Module):\n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size, n_embd)\n        self.ffwd = FeedForward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = self.ln1(self.sa(x)) + x\n        return self.ln2(self.ffwd(x)) + x\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, n_head, head_size, n_embd): \n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_head)])\n        self.proj = nn.Linear(n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = torch.cat([h(x) for h in self.heads], dim=-1)\n        x = self.dropout(x)\n        return self.proj(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T20:57:25.364680Z","iopub.execute_input":"2024-11-06T20:57:25.365033Z","iopub.status.idle":"2024-11-06T20:57:25.391311Z","shell.execute_reply.started":"2024-11-06T20:57:25.364999Z","shell.execute_reply":"2024-11-06T20:57:25.390251Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = BigramLanguageModel()\nmodel = model.to(device)\nprint(f\"Total number of parameters: {sum(p.numel() for p in model.parameters())}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T20:57:25.950903Z","iopub.execute_input":"2024-11-06T20:57:25.951582Z","iopub.status.idle":"2024-11-06T20:57:25.975653Z","shell.execute_reply.started":"2024-11-06T20:57:25.951538Z","shell.execute_reply":"2024-11-06T20:57:25.974787Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=lr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T19:50:51.291074Z","iopub.execute_input":"2024-11-06T19:50:51.291856Z","iopub.status.idle":"2024-11-06T19:50:51.298020Z","shell.execute_reply.started":"2024-11-06T19:50:51.291815Z","shell.execute_reply":"2024-11-06T19:50:51.297077Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for iter in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n    xb = xb.to(device)\n    yb = yb.to(device)\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T19:50:51.960275Z","iopub.execute_input":"2024-11-06T19:50:51.960667Z","iopub.status.idle":"2024-11-06T19:55:01.343631Z","shell.execute_reply.started":"2024-11-06T19:50:51.960630Z","shell.execute_reply":"2024-11-06T19:55:01.342510Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(model.generate(context, max_new_tokens=2000)[0].tolist()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T19:57:07.557735Z","iopub.execute_input":"2024-11-06T19:57:07.558124Z","iopub.status.idle":"2024-11-06T19:57:23.400121Z","shell.execute_reply.started":"2024-11-06T19:57:07.558087Z","shell.execute_reply":"2024-11-06T19:57:23.399185Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\nfor iter in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n    xb = xb.to(device)\n    yb = yb.to(device)\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T20:00:04.647472Z","iopub.execute_input":"2024-11-06T20:00:04.647850Z","iopub.status.idle":"2024-11-06T20:04:13.175044Z","shell.execute_reply.started":"2024-11-06T20:00:04.647816Z","shell.execute_reply":"2024-11-06T20:04:13.174020Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(model.generate(context, max_new_tokens=2000)[0].tolist()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T20:04:38.371775Z","iopub.execute_input":"2024-11-06T20:04:38.372574Z","iopub.status.idle":"2024-11-06T20:04:54.654480Z","shell.execute_reply.started":"2024-11-06T20:04:38.372531Z","shell.execute_reply":"2024-11-06T20:04:54.653490Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=lr*0.25)\nfor iter in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n    xb = xb.to(device)\n    yb = yb.to(device)\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T20:05:14.418634Z","iopub.execute_input":"2024-11-06T20:05:14.419417Z","iopub.status.idle":"2024-11-06T20:09:21.593385Z","shell.execute_reply.started":"2024-11-06T20:05:14.419376Z","shell.execute_reply":"2024-11-06T20:09:21.592369Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(model.generate(context, max_new_tokens=2000)[0].tolist()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T20:11:11.640931Z","iopub.execute_input":"2024-11-06T20:11:11.641357Z","iopub.status.idle":"2024-11-06T20:11:27.647731Z","shell.execute_reply.started":"2024-11-06T20:11:11.641306Z","shell.execute_reply":"2024-11-06T20:11:27.646782Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training more complex model","metadata":{}},{"cell_type":"code","source":"batch_size = 64\nblock_size = 256\n\ndef get_batch(split):\n    data = train_data if split == 'train' else val_data\n\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    \n    return x, y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T20:12:09.273314Z","iopub.execute_input":"2024-11-06T20:12:09.273695Z","iopub.status.idle":"2024-11-06T20:12:09.282563Z","shell.execute_reply.started":"2024-11-06T20:12:09.273659Z","shell.execute_reply":"2024-11-06T20:12:09.281567Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmax_iters = 5000\neval_interval = 500\nlr = 3e-4\neval_iters = 200\nn_embd = 384\nn_head = 6\nn_layer = 6\ndropout = 0.2\n\nclass FeedForward(nn.Module):\n    def __init__(self, n_embd): \n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(n_embd, n_embd * 4), \n                                 nn.ReLU(),\n                                 nn.Linear(4 * n_embd, n_embd), \n                                 nn.Dropout(dropout)\n                                )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass GPT(nn.Module):\n    def __init__(self): \n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, n_embd)\n        self.pos_enc = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.lm_f = nn.LayerNorm(n_embd)\n        self.lm_layer = nn.Linear(n_embd, vocab_size)\n        \n    def forward(self, idx, targets=None):\n        # idx shape (B, T)\n        B, T = idx.shape\n        token_embd = self.embed(idx)\n        pos_embd = self.pos_enc(torch.arange(T, device=device))\n        x = token_embd + pos_embd\n        x = self.blocks(x)\n        x = self.lm_f(x)\n        logits = self.lm_layer(x)\n        \n        if targets is None:\n            loss = None\n        else:\n            #logits shape (B, T), targets shape (B, T)\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -block_size:]\n            logits, loss = self(idx_cond)\n            logits = logits[:,-1,:]\n            probs = F.softmax(logits, dim=1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat([idx, idx_next], dim=1)\n\n        return idx\n\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n    def forward(self, x):\n        # input of size (batch, time-step, channels)\n        # output of size (batch, time-step, head size)\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,hs)\n        q = self.query(x) # (B,T,hs)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,hs)\n        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n        return out\n        \n        \nclass Block(nn.Module):\n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size, n_embd)\n        self.ffwd = FeedForward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = self.ln1(self.sa(x)) + x\n        return self.ln2(self.ffwd(x)) + x\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, n_head, head_size, n_embd): \n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_head)])\n        self.proj = nn.Linear(n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = torch.cat([h(x) for h in self.heads], dim=-1)\n        x = self.dropout(x)\n        return self.proj(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T20:55:32.397214Z","iopub.execute_input":"2024-11-06T20:55:32.397706Z","iopub.status.idle":"2024-11-06T20:55:32.432931Z","shell.execute_reply.started":"2024-11-06T20:55:32.397655Z","shell.execute_reply":"2024-11-06T20:55:32.431933Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = GPT()\nmodel = model.to(device)\nprint(f\"Total number of parameters: {sum(p.numel() for p in model.parameters())}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T20:55:38.227055Z","iopub.execute_input":"2024-11-06T20:55:38.227474Z","iopub.status.idle":"2024-11-06T20:55:38.383211Z","shell.execute_reply.started":"2024-11-06T20:55:38.227435Z","shell.execute_reply":"2024-11-06T20:55:38.382220Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\nfor iter in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, diff loss {losses['val'] - losses['train']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n    xb = xb.to(device)\n    yb = yb.to(device)\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T20:14:07.821877Z","iopub.execute_input":"2024-11-06T20:14:07.822728Z","iopub.status.idle":"2024-11-06T20:43:39.878670Z","shell.execute_reply.started":"2024-11-06T20:14:07.822690Z","shell.execute_reply":"2024-11-06T20:43:39.877576Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(model.generate(context, max_new_tokens=2000)[0].tolist()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T20:49:06.138917Z","iopub.execute_input":"2024-11-06T20:49:06.139341Z","iopub.status.idle":"2024-11-06T20:49:44.504498Z","shell.execute_reply.started":"2024-11-06T20:49:06.139306Z","shell.execute_reply":"2024-11-06T20:49:44.503515Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 32\nblock_size = 512\n\ndef get_batch(split):\n    data = train_data if split == 'train' else val_data\n\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    \n    return x, y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T21:06:55.518281Z","iopub.execute_input":"2024-11-06T21:06:55.518727Z","iopub.status.idle":"2024-11-06T21:06:55.525373Z","shell.execute_reply.started":"2024-11-06T21:06:55.518688Z","shell.execute_reply":"2024-11-06T21:06:55.524432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmax_iters = 5000\neval_interval = 500\nlr = 3e-4\neval_iters = 200\nn_embd = 384\nn_head = 6\nn_layer = 10\ndropout = 0.2\n\nclass FeedForward(nn.Module):\n    def __init__(self, n_embd): \n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(n_embd, n_embd * 4), \n                                 nn.ReLU(),\n                                 nn.Linear(4 * n_embd, n_embd), \n                                 nn.Dropout(dropout)\n                                )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass GPT(nn.Module):\n    def __init__(self): \n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, n_embd)\n        self.pos_enc = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.lm_f = nn.LayerNorm(n_embd)\n        self.lm_layer = nn.Linear(n_embd, vocab_size)\n        \n    def forward(self, idx, targets=None):\n        # idx shape (B, T)\n        B, T = idx.shape\n        token_embd = self.embed(idx)\n        pos_embd = self.pos_enc(torch.arange(T, device=device))\n        x = token_embd + pos_embd\n        x = self.blocks(x)\n        x = self.lm_f(x)\n        logits = self.lm_layer(x)\n        \n        if targets is None:\n            loss = None\n        else:\n            #logits shape (B, T), targets shape (B, T)\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -block_size:]\n            logits, loss = self(idx_cond)\n            logits = logits[:,-1,:]\n            probs = F.softmax(logits, dim=1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat([idx, idx_next], dim=1)\n\n        return idx\n\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        # input of size (batch, time-step, channels)\n        # output of size (batch, time-step, head size)\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,hs)\n        q = self.query(x) # (B,T,hs)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,hs)\n        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n        return out\n        \n        \nclass Block(nn.Module):\n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size, n_embd)\n        self.ffwd = FeedForward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = self.ln1(self.sa(x)) + x\n        return self.ln2(self.ffwd(x)) + x\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, n_head, head_size, n_embd): \n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_head)])\n        self.proj = nn.Linear(n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = torch.cat([h(x) for h in self.heads], dim=-1)\n        x = self.dropout(x)\n        return self.proj(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T21:50:07.400133Z","iopub.execute_input":"2024-11-06T21:50:07.400754Z","iopub.status.idle":"2024-11-06T21:50:07.426493Z","shell.execute_reply.started":"2024-11-06T21:50:07.400714Z","shell.execute_reply":"2024-11-06T21:50:07.425513Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = GPT()\nmodel = model.to(device)\nprint(f\"Total number of parameters: {sum(p.numel() for p in model.parameters())}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T21:50:15.179717Z","iopub.execute_input":"2024-11-06T21:50:15.180624Z","iopub.status.idle":"2024-11-06T21:50:15.481452Z","shell.execute_reply.started":"2024-11-06T21:50:15.180568Z","shell.execute_reply":"2024-11-06T21:50:15.480447Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\nfor iter in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, diff loss {losses['val'] - losses['train']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n    xb = xb.to(device)\n    yb = yb.to(device)\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T22:39:27.987761Z","iopub.execute_input":"2024-11-06T22:39:27.988727Z","iopub.status.idle":"2024-11-06T23:05:19.452835Z","shell.execute_reply.started":"2024-11-06T22:39:27.988676Z","shell.execute_reply":"2024-11-06T23:05:19.451482Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(model.generate(context, max_new_tokens=2000)[0].tolist()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T23:25:55.954258Z","iopub.execute_input":"2024-11-06T23:25:55.954713Z","iopub.status.idle":"2024-11-06T23:26:59.089796Z","shell.execute_reply.started":"2024-11-06T23:25:55.954665Z","shell.execute_reply":"2024-11-06T23:26:59.088797Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 64\nblock_size = 256\n\ndef get_batch(split):\n    data = train_data if split == 'train' else val_data\n\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    \n    return x, y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T15:58:31.336645Z","iopub.execute_input":"2024-11-07T15:58:31.337010Z","iopub.status.idle":"2024-11-07T15:58:31.343841Z","shell.execute_reply.started":"2024-11-07T15:58:31.336976Z","shell.execute_reply":"2024-11-07T15:58:31.342943Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmax_iters = 5000\neval_interval = 500\nlr = 3e-4\neval_iters = 200\nn_embd = 256\nn_head = 4\nn_layer = 4\ndropout = 0.0\n\nclass FeedForward(nn.Module):\n    def __init__(self, n_embd): \n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(n_embd, n_embd * 4), \n                                 nn.ReLU(),\n                                 nn.Linear(4 * n_embd, n_embd), \n                                 nn.Dropout(dropout)\n                                )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass GPT(nn.Module):\n    def __init__(self): \n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, n_embd)\n        self.pos_enc = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.lm_f = nn.LayerNorm(n_embd)\n        self.lm_layer = nn.Linear(n_embd, vocab_size)\n        \n    def forward(self, idx, targets=None):\n        # idx shape (B, T)\n        B, T = idx.shape\n        token_embd = self.embed(idx)\n        pos_embd = self.pos_enc(torch.arange(T, device=device))\n        x = token_embd + pos_embd\n        x = self.blocks(x)\n        x = self.lm_f(x)\n        logits = self.lm_layer(x)\n        \n        if targets is None:\n            loss = None\n        else:\n            #logits shape (B, T), targets shape (B, T)\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -block_size:]\n            logits, loss = self(idx_cond)\n            logits = logits[:,-1,:]\n            probs = F.softmax(logits, dim=1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat([idx, idx_next], dim=1)\n\n        return idx\n\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        # input of size (batch, time-step, channels)\n        # output of size (batch, time-step, head size)\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,hs)\n        q = self.query(x) # (B,T,hs)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,hs)\n        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n        return out\n        \n        \nclass Block(nn.Module):\n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size, n_embd)\n        self.ffwd = FeedForward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = self.ln1(self.sa(x)) + x\n        return self.ln2(self.ffwd(x)) + x\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, n_head, head_size, n_embd): \n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_head)])\n        self.proj = nn.Linear(n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = torch.cat([h(x) for h in self.heads], dim=-1)\n        x = self.dropout(x)\n        return self.proj(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T16:08:27.897966Z","iopub.execute_input":"2024-11-07T16:08:27.898352Z","iopub.status.idle":"2024-11-07T16:08:27.923854Z","shell.execute_reply.started":"2024-11-07T16:08:27.898304Z","shell.execute_reply":"2024-11-07T16:08:27.923113Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"model = GPT()\nmodel = model.to(device)\nprint(f\"Total number of parameters: {sum(p.numel() for p in model.parameters())}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T16:08:28.552655Z","iopub.execute_input":"2024-11-07T16:08:28.553021Z","iopub.status.idle":"2024-11-07T16:08:28.604115Z","shell.execute_reply.started":"2024-11-07T16:08:28.552985Z","shell.execute_reply":"2024-11-07T16:08:28.603182Z"}},"outputs":[{"name":"stdout","text":"Total number of parameters: 3255361\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\nfor iter in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, diff loss {losses['val'] - losses['train']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n    xb = xb.to(device)\n    yb = yb.to(device)\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T16:08:30.526531Z","iopub.execute_input":"2024-11-07T16:08:30.527797Z","iopub.status.idle":"2024-11-07T16:18:30.277208Z","shell.execute_reply.started":"2024-11-07T16:08:30.527710Z","shell.execute_reply":"2024-11-07T16:18:30.276275Z"}},"outputs":[{"name":"stdout","text":"step 0: train loss 4.4203, val loss 4.4114, diff loss -0.0089\nstep 500: train loss 1.8011, val loss 1.9257, diff loss 0.1246\nstep 1000: train loss 1.4433, val loss 1.6390, diff loss 0.1957\nstep 1500: train loss 1.3189, val loss 1.5643, diff loss 0.2454\nstep 2000: train loss 1.2401, val loss 1.5294, diff loss 0.2892\nstep 2500: train loss 1.1749, val loss 1.5437, diff loss 0.3688\nstep 3000: train loss 1.1095, val loss 1.5595, diff loss 0.4500\nstep 3500: train loss 1.0538, val loss 1.5958, diff loss 0.5419\nstep 4000: train loss 0.9968, val loss 1.6579, diff loss 0.6611\nstep 4500: train loss 0.9399, val loss 1.7307, diff loss 0.7908\nstep 4999: train loss 0.8814, val loss 1.8100, diff loss 0.9286\n","output_type":"stream"}],"execution_count":63},{"cell_type":"markdown","source":"p = 0.2\nstep 0: train loss 4.3143, val loss 4.3112, diff loss -0.0031\r\nstep 500: train loss 1.9058, val loss 2.0208, diff loss 0.1151\r\nstep 1000: train loss 1.4890, val loss 1.6842, diff loss 0.1952\r\nstep 1500: train loss 1.3414, val loss 1.5590, diff loss 0.2176\r\nstep 2000: train loss 1.2543, val loss 1.5117, diff loss 0.2575\r\nstep 2500: train loss 1.1992, val loss 1.4886, diff loss 0.2894\r\nstep 3000: train loss 1.1495, val loss 1.4826, diff loss 0.3332\r\nstep 3500: train loss 1.1087, val loss 1.4855, diff loss\n\np = 0.5\nstep 0: train loss 1.2580, val loss 1.5604, diff loss 0.3023\r\nstep 500: train loss 1.2424, val loss 1.5538, diff loss 0.3114\r\nstep 1000: train loss 1.2214, val loss 1.5448, diff loss 0.3234\r\nstep 1500: train loss 1.2074, val loss 1.5378, diff loss 0.3304\r\nstep 2000: train loss 1.1869, val loss 1.5287, diff loss 0.3419\r\nstep 2500: train loss 1.1825, val loss 1.5419, diff loss 0.3595\r\nstep 3000: train loss 1.1648, val loss 1.5370, diff loss 0.3722\r\nstep 3500: train loss 1.1543, val loss 1.5312, diff loss 0.3768\r\nstep 4000: train loss 1.1419, val loss 1.5389, diff loss 0.3970\r\nstep 4500: train loss 1.1299, val loss 1.5438, diff loss 0.4139\r\nstep 4999: train loss 1.1194, val loss 1.5455, diff l\n\nt = 512\nstep 0: train loss 4.3666, val loss 4.3507, diff loss -0.0159\r\nstep 500: train loss 2.3006, val loss 2.3596, diff loss 0.0590\r\nstep 1000: train loss 1.5622, val loss 1.7534, diff loss 0.1912\r\nstep 1500: train loss 1.3480, val loss 1.5809, diff loss 0.2329\r\nstep 2000: train loss 1.2438, val loss 1.5013, diff loss 0.2576\r\nstep 2500: train loss 1.1666, val loss 1.4814, diff loss 0.3149\r\nstep 3000: train loss 1.1061, val loss 1.4795, diff loss 0.3734\r\nstep 3500: train loss 1.0529, val loss 1.4962, diff loss 0.4433\r\nstep 4000: train loss 0.9921, val loss 1.5086, diff los\n\nn_layers = 8\nstep 0: train loss 4.4640, val loss 4.4617, diff loss -0.0023\r\nstep 500: train loss 2.0043, val loss 2.0823, diff loss 0.0780\r\nstep 1000: train loss 1.5414, val loss 1.7261, diff loss 0.1847\r\nstep 1500: train loss 1.3615, val loss 1.5778, diff loss 0.2163\r\nstep 2000: train loss 1.2767, val loss 1.5239, diff loss 0.2471\r\nstep 2500: train loss 1.2151, val loss 1.4999, diff loss 0.2848\r\nstep 3000: train loss 1.1659, val loss 1.4918, diff loss 0.3259\r\nstep 3500: train loss 1.1199, val loss 1.4969, diff loss 0.3770\r\nstep 4000: train loss 1.0873, val loss 1.5112, diff loss 0.4240\r\nstep 4500: train loss 1.0492, val loss 1.5077, diff loss 0.4585\r\nstep 4999: train loss 1.0182, val loss 1.5255, diff l\n\nn_embd = 512\nn_head = 8\nstep 0: train loss 4.3457, val loss 4.3480, diff loss 0.0023\r\nstep 500: train loss 1.9286, val loss 2.0417, diff loss 0.1132\r\nstep 1000: train loss 1.4955, val loss 1.6967, diff loss 0.2012\r\nstep 1500: train loss 1.3296, val loss 1.5483, diff loss 0.2187\r\nstep 2000: train loss 1.2450, val loss 1.5145, diff loss 0.2695\r\nstep 2500: train loss 1.1842, val loss 1.4963, diff loss 0.3122\r\nstep 3000: train loss 1.1347, val loss 1.5023, diff loss 0.3676\r\nstep 3500: train loss 1.0872, val loss 1.5024, diff loss 0.4152\r\nstep 4000: train loss 1.0414, val loss 1.5185, diff loss 0.4770\r\nstep 4500: train loss 0.9959, val loss 1.5341, diff loss 0.5382\r\nstep 4999: train loss 0.9538, val loss 1.5756, diff losn_embd = 512\nn_head = 8\nn_layers = 8\nstep 0: train loss 4.1405, val loss 4.1471, diff loss 0.0067\r\nstep 500: train loss 2.1302, val loss 2.1906, diff loss 0.0604\r\nstep 1000: train loss 1.7153, val loss 1.8802, diff loss 0.1649\r\nstep 1500: train loss 1.5075, val loss 1.7024, diff loss 0.1949\r\nstep 2000: train loss 1.4135, val loss 1.6379, diff loss 0.2244\r\nstep 2500: train loss 1.3447, val loss 1.5705, diff loss 0.2258\r\nstep 3000: train loss 1.2980, val loss 1.5476, diff loss 0.2496\r\nstep 3500: train loss 1.2620, val loss 1.5472, diff loss 0.2852\r\nstep 4000: train loss 1.2308, val loss 1.5219, diff loss 0.2912\r\nstep 4500: train loss 1.2109, val loss 1.5257, diff loss 0.3147\r\nstep 4999: train loss 1.1836, val loss 1.5191, diff l\nstep 5000: train loss 1.1828, val loss 1.5241, diff loss 0.3413\r\nstep 5500: train loss 1.1583, val loss 1.5287, diff loss 0.3704\r\nstep 6000: train loss 1.1532, val loss 1.5283, diff loss 0.3751\r\nstep 6500: train loss 1.1392, val loss 1.5434, diff loss 0.4041\r\nstep 7000: train loss 1.1477, val loss 1.5469, diff loss 0.\n\nn_embd = 320\nn_head = 5\nn_layer = 4\nstep 0: train loss 4.3636, val loss 4.3533, diff loss -0.0103\r\nstep 500: train loss 1.9883, val loss 2.0751, diff loss 0.0869\r\nstep 1000: train loss 1.5644, val loss 1.7555, diff loss 0.1911\r\nstep 1500: train loss 1.4171, val loss 1.6312, diff loss 0.2141\r\nstep 2000: train loss 1.3365, val loss 1.5639, diff loss 0.2274\r\nstep 2500: train loss 1.2792, val loss 1.5238, diff loss 0.2445\r\nstep 3000: train loss 1.2340, val loss 1.5031, diff loss 0.2691\r\nstep 3500: train loss 1.2043, val loss 1.4952, diff loss 0.2909\r\nstep 4000: train loss 1.1728, val loss 1.4864, diff lossstep 4500: train loss 1.1506, val loss 1.4823, diff loss 0.3317\n\n\nn_embd = 256\nn_head = 4\nn_layer = 4\nstep 0: train loss 4.4203, val loss 4.4114, diff loss -0.0089\r\nstep 500: train loss 2.0601, val loss 2.1233, diff loss 0.0633\r\nstep 1000: train loss 1.6193, val loss 1.8052, diff loss 0.1859\r\nstep 1500: train loss 1.4730, val loss 1.6747, diff loss 0.2017\r\nstep 2000: train loss 1.3984, val loss 1.6009, diff loss 0.2025\r\nstep 2500: train loss 1.3416, val loss 1.5605, diff loss 0.2188\r\nstep 3000: train loss 1.3044, val loss 1.5421, diff loss 0.2377\r\nstep 3500: train loss 1.2759, val loss 1.5158, diff loss 0.2399\r\nstep 4000: train loss 1.2507, val loss 1.5030, diff loss 0.2522\r\nstep 4500: train loss 1.2286, val loss 1.4956, diff loss 0.2670\r\nstep 4999: train loss 1.2137, val loss 1.4893, diff l\n\nn_embd = 256\nn_head = 4\nn_layer = 4\np = 0.5\nstep 0: train loss 4.4203, val loss 4.4114, diff loss -0.0089\r\nstep 500: train loss 2.4102, val loss 2.4385, diff loss 0.0283\r\nstep 1000: train loss 2.0076, val loss 2.1037, diff loss 0.0961\r\nstep 1500: train loss 1.7711, val loss 1.9379, diff loss 0.1668\r\nstep 2000: train loss 1.6449, val loss 1.8469, diff loss 0.2020\r\nstep 2500: train loss 1.5680, val loss 1.7812, diff loss 0.2132\r\nstep 3000: train loss 1.5124, val loss 1.7330, diff loss 0.2206\r\nstep 3500: train loss 1.4739, val loss 1.6934, diff loss 0.2195\r\nstep 4000: train loss 1.4497, val loss 1.6768, diff lossstep 4500: train loss 1.4219, val loss 1.6559, diff loss 0.2340\n\nn_embd = 256\nn_head = 4\nn_layer = 4\np = 0.0\n 0.2270\noss 0.2756 0.3136\n3992oss 0.3355s 0.6219\noss 0.5073s 0.5165\noss 0.4261 0.3768","metadata":{}},{"cell_type":"code","source":"# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(model.generate(context, max_new_tokens=500)[0].tolist()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T16:28:14.657491Z","iopub.execute_input":"2024-11-07T16:28:14.657863Z","iopub.status.idle":"2024-11-07T16:28:18.602070Z","shell.execute_reply.started":"2024-11-07T16:28:14.657827Z","shell.execute_reply":"2024-11-07T16:28:18.601074Z"}},"outputs":[{"name":"stdout","text":"\nMen punished again.\n\nAEdile:\nShame Gremions, much more! Well, ready, we must have\nhad looked on my cousin's point.\n\nDUKE OF YORK:\nProvoke poisonseth be so moved: if he much\nmerciful villain! Gloucester, after those thoughts\nthat he shall be chequeen, make her heart.\n\nDUKE OF AUMERLE:\nComfort will it is the walks.\n\nKING RICHARD II:\nNorfolk, know thy name, dear?\nFerewell, of this day of mischa harm, which and like.\nWhat came unto thee were to come his wrong,\nOr who do imposset in fore; you were us\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}