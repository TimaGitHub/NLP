{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install einops","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import einops\nimport torch\nimport math\nimport torch.nn as nn\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-10-27T13:42:35.550784Z","iopub.execute_input":"2024-10-27T13:42:35.551190Z","iopub.status.idle":"2024-10-27T13:42:35.556993Z","shell.execute_reply.started":"2024-10-27T13:42:35.551154Z","shell.execute_reply":"2024-10-27T13:42:35.555328Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"x = torch.randn(16, 100, dim)\nto_qvk = torch.nn.Linear(dim, dim * 3, bias=False)\nqvk = to_qvk(x)\nq, v, k = tuple(einops.rearrange(qvk, \"b t (n w) -> n b t w\", n=3))\nmask = torch.triu(torch.ones((q.shape[1], q.shape[1])), diagonal=1).bool()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T13:22:17.814283Z","iopub.execute_input":"2024-10-27T13:22:17.814854Z","iopub.status.idle":"2024-10-27T13:22:17.827807Z","shell.execute_reply.started":"2024-10-27T13:22:17.814703Z","shell.execute_reply":"2024-10-27T13:22:17.826723Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"dot_prod = torch.einsum(\"b i e, b j e -> b i j\", q, k)\nscaled_dot_prod = dot_prod * scale_factor\nif mask is not None:\n    assert mask.shape == scaled_dot_prod.shape[1:]\n    scaled_dot_prod = scaled_dot_prod.masked_fill(mask, -np.inf)\nscaled_dot_prod_norm = torch.softmax(scaled_dot_prod, dim=-1)\nattention = torch.einsum(\"b i e, b e j -> b i j\", scaled_dot_prod_norm, v)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T13:22:22.859465Z","iopub.execute_input":"2024-10-27T13:22:22.859912Z","iopub.status.idle":"2024-10-27T13:22:22.869370Z","shell.execute_reply.started":"2024-10-27T13:22:22.859873Z","shell.execute_reply":"2024-10-27T13:22:22.868324Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"class SelfAttention(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.to_qkv = torch.nn.Linear(dim, dim * 3, bias=False)\n        self.scale_factor = 1 / math.sqrt(dim)\n        \n    def forward(self, x, mask=None):\n        assert x.dim() == 3\n        \n        qvk = to_qvk(x)\n        q, v, k = tuple(einops.rearrange(qvk, \"b t (n e) -> n b t e\", n=3))\n        dot_prod = torch.einsum(\"b i e, b j e -> b i j\", q, k)\n        scaled_dot_prod = dot_prod * scale_factor\n        if mask:\n            mask = torch.triu(torch.ones((q.shape[1], q.shape[1])), diagonal=1).bool()\n            scaled_dot_prod = scaled_dot_prod.masked_fill(mask, -np.inf)\n        scaled_dot_prod_norm = torch.softmax(scaled_dot_prod, dim=-1)\n        attention = torch.einsum(\"b i e, b e j -> b i j\", scaled_dot_prod_norm, v)\n        \n        return attention\n    \n    \nclass MultiHeadAttention(nn.Module):\n    def __init__(self, dim, heads):\n        super().__init__()\n        assert dim % heads == 0 \n        self.to_qkv = torch.nn.Linear(dim, dim * 3, bias=False)\n        self.scale_factor = 1 / math.sqrt(dim / heads)\n        self.heads = heads\n        self.W_0 = torch.nn.Linear(dim, dim, bias=False)\n        \n    def forward(self, x, mask=None):\n        assert x.dim() == 3\n        \n        qvk = to_qvk(x)\n        q, v, k = tuple(einops.rearrange(qvk, \"b t (n h e) -> n b h t e\", n=3, h=self.heads))\n        dot_prod = torch.einsum(\"b h i e, b h j e -> b h i j\", q, k)\n        scaled_dot_prod = dot_prod * scale_factor\n        if mask:\n            mask = torch.triu(torch.ones((q.shape[2], q.shape[2])), diagonal=1).bool()\n            scaled_dot_prod = scaled_dot_prod.masked_fill(mask, -np.inf)\n        scaled_dot_prod_norm = torch.softmax(scaled_dot_prod, dim=-1)\n        attention = torch.einsum(\"b h i e, b h e j -> b h i j\", scaled_dot_prod_norm, v)\n        out = einops.rearrange(attetion, \"b h t d -> b t (h d)\")\n        \n        return self.W_O(out)\n        ","metadata":{"execution":{"iopub.status.busy":"2024-10-27T14:25:51.456001Z","iopub.execute_input":"2024-10-27T14:25:51.456829Z","iopub.status.idle":"2024-10-27T14:25:51.470406Z","shell.execute_reply.started":"2024-10-27T14:25:51.456787Z","shell.execute_reply":"2024-10-27T14:25:51.469236Z"},"trusted":true},"execution_count":144,"outputs":[]},{"cell_type":"code","source":"class TranformerBlock(nn.Module):\n    def __init__(self, dim, heads, dim_linear_block=1024, dropout=0.1):\n        super().__init__()\n        \n        self.mha = MultiHeadAttention(dim, heads)\n        self.dropout = nn.DropOut(dropout)\n        self.ln1 = nn.LayerNorm(dim)\n        self.ln2 = nn.LayerNorm(dim)\n        \n        self.linear = nn.Sequential(\n            nn.Linear(dim, dim_linear_block),\n            nn.GELU(),\n            nn.DropOut(dropout),\n            nn.Linear(dim_linear_block, dim),\n            nn.DropOut(dropout)\n        )\n        \n    def forward(self, x, mask=None):\n        x = self.ln1(self.drop(self.mha(x, mask)) + x)\n        return self.ln2(self.linear(x) + x)\n        \n        \nclass TransformerEncoder(nn.Module):\n    def __init__(self, dim, blocks=6, heads=8):\n        super().__init__()\n        self.block_list = [TransformerBlock(dim, heads) for _ in range(blocks)]\n        self.layers = nn.ModuleList(self.block_list)\n\n    def forward(self, x, mask=None):\n        for layer in self.layers:\n            x = layer(x, mask)\n        return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}