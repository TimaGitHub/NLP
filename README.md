

# üó£Ô∏è **Natural Language Processing Repository** üß†  

Welcome to the **NLP Repository** ‚Äî a comprehensive collection of **research papers**, **code implementations**, and **scripts** for key NLP topics, including Attention, RNNs, and Transformers. This repository serves as a hub for both foundational knowledge and practical applications.

---

## üìÇ **Repository Structure**

The repository is organized into folders covering major NLP themes:

| Folder Name        | Description                                                                 |
|--------------------|-----------------------------------------------------------------------------|
| **Attention**      | Key models and implementations based on the **Attention mechanism**.       |
| **RNN**            | Implementations of **Recurrent Neural Networks (RNNs)** for NLP tasks.     |
| **Transformers**   | Architectures and techniques based on the Transformer model, including **GPT, BERT**, and more. |

---

## üîç **Folder Details**

### üîπ **Attention**
This folder focuses on the **Attention mechanism**, a foundational concept for modern NLP models.  

**Contents**:  
- **Papers**:  
  - *Attention Is All You Need*  
  - *Attention*  

- **Code Implementations**:  
  - `bigramlm.ipynb`: A notebook implementing a simple **Bigram Language Model** with attention.  

---

### üîπ **RNN**  
Explore classical **Recurrent Neural Networks** and their role in sequence modeling.  

**Contents**:  
- **Papers**:  
  - *The Unreasonable Effectiveness of RNNs*  
  - *A Neural Probabilistic Language Model*  
  - *Sequence to Sequence Learning with Neural Networks*  

- **Code Implementations**:  
  - `bigramlm-with-rnn.ipynb`: A bigram language model implemented using RNN.  
  - `machine-translation.ipynb`: Sequence-to-sequence model for machine translation.  

---

### üîπ **Transformers**  
This folder focuses on modern NLP architectures derived from the **Transformer** model.  

**Contents**:  
- **Papers**:  
  - *BERT*: Bidirectional Encoder Representations from Transformers.  
  - *RoBERTa*: Robustly optimized BERT for NLP tasks.  
  - *MultiQuery+MultiHeadf*: Multi-query and multi-head attention.  
  - *What Do Position Embeddings Learn*.  
  - *Understanding einsum for Deep learning*.  

- **Code Implementations**:  
  - `BERT_Fine_Tuning_Sentence_Classification.ipynb`: Fine-tuning BERT for text classification.  
  - `chatgpt-demo-architecture-test.ipynb`: Example notebook for GPT-based architecture.  
  - `gpt-tokenizer.ipynb`: Tokenization demonstration for GPT models.  
  - `pretrained-gpt-2.ipynb`: Working with GPT-2 pre-trained models.  

- **Inference Scripts**:  
Located in `lamma-2-inference/`:  
  - `model.py`: Defines the Transformer model.  
  - `inference.py`: Inference script for running the **LLaMA-2** model.  

---

